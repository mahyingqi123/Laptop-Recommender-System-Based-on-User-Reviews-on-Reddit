{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabsa import AspectTermExtraction as ATEPC\n",
    "from pyabsa import DatasetItem\n",
    "import warnings\n",
    "from pyabsa import ModelSaveOption, DeviceTypeOption\n",
    "config = (\n",
    "    ATEPC.ATEPCConfigManager.get_atepc_config_english()\n",
    ")\n",
    "config.model = ATEPC.ATEPCModelList.BERT_BASE_ATEPC  # improved version of LCF-ATEPC\n",
    "dataset = ATEPC.ATEPCDatasetList.Laptop14\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "config.max_seq_len = 300\n",
    "config.batch_size = 16\n",
    "config.patience = 2\n",
    "config.log_step = -1\n",
    "config.seed = [1, 2, 3]\n",
    "config.verbose = False  # If verbose == True, PyABSA will output the model strcture and seversal processed data examples\n",
    "config.notice = (\n",
    "    \"This is an model for aspect term extraction\"  # for memos usage\n",
    ")\n",
    "\n",
    "trainer = ATEPC.ATEPCTrainer(\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    auto_device=DeviceTypeOption.AUTO,  # use cuda if available\n",
    "    checkpoint_save_mode=ModelSaveOption.SAVE_MODEL_STATE_DICT,  # save state dict only instead of the whole model\n",
    "    load_aug=False,  # there are some augmentation dataset for integrated datasets, you use them by setting load_aug=True to improve performance\n",
    "    path_to_save=\"ate_model_2\"\n",
    ")\n",
    "import torch\n",
    "print(torch.__version__ )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
